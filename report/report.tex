\documentclass[a4paper]{scrartcl}

\usepackage[a4paper, left=1.8cm, right=1.8cm, top=2.5cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{scrlayer-scrpage}
\usepackage{xcolor}
\usepackage{hyperref}

\renewcommand{\arraystretch}{1.4}
\pagestyle{scrheadings}
\automark{section}
\ohead{\pagemark}
\cfoot{}

% --- Document Info ---
\title{ML-Enhanced Large Neighborhood Search \\ Project Report}
\subtitle{Machine Learning for Optimization, WS 2025}
\author{
    Daniel Levin (12433760)
}
\date{}

\begin{document}
\maketitle

% ============================================================
% Problem Description
% ============================================================
\section{Problem: SCF-PDP}

The \textbf{Selective Capacitated Fair Pickup and Delivery Problem (SCF-PDP)} is a vehicle routing problem
where the goal is to design fair and feasible routes for a subset of customer requests.
Each customer needs transportation of goods from a pickup location to a corresponding drop-off location.

The problem is modeled on a complete directed graph $G = (V, A)$ where:
\begin{itemize}
    \item $V$ contains the vehicle depot plus all pickup/drop-off locations
    \item $A = \{(u,v) : u,v \in V, u \neq v\}$ represents travel routes
    \item Arc distances $a_{u,v} = \lceil\sqrt{(x_u - x_v)^2 + (y_u - y_v)^2}\rceil$ (rounded Euclidean)
\end{itemize}

There are $n$ customer requests $CR = \{1, \ldots, n\}$, each defined by pickup location $v_i^\uparrow$ and
drop-off location $v_i^\downarrow$ with demand $c_i$.
A fleet $K$ of $n_K$ identical vehicles with capacity $C$ serves requests starting from the depot.

A feasible solution assigns routes $R_k$ to each vehicle $k \in K$ such that:
\begin{itemize}
    \item Vehicle capacity never exceeded along any route
    \item Each served request handled entirely by one vehicle
    \item At least $\gamma$ requests served across all vehicles
\end{itemize}

The objective minimizes total distance plus fairness penalty:
\[
\sum_{k \in K} d(R_k) + \rho \cdot (1 - J(R))
\]
where $d(R_k)$ is route $k$'s total distance and $J(R)$ is the Jain fairness index:
\[
J(R) = \frac{\left(\sum_{k \in K} d(R_k)\right)^2}{n_K \cdot \sum_{k \in K} d(R_k)^2}
\]
The fairness index $J(R) \in (0, 1]$ equals 1 when all routes have equal distance.
Parameter $\rho$ controls the distance-fairness trade-off.

% ============================================================
% Baseline ALNS
% ============================================================
\section{Baseline: ALNS}

The baseline implementation uses Adaptive Large Neighborhood Search (ALNS) with simulated annealing acceptance,
developed and tuned in the Heuristic Optimization Techniques course.
This section summarizes the key components.

\subsection{Operators}

The implementation uses 3 destroy and 3 repair operators, yielding 9 combinations.
Each destroy operator removes $q$ requests sampled uniformly from $[10\%, 40\%]$ of served requests.

\textbf{Destroy Operators:}
\begin{itemize}
    \item \textbf{Random}: Removes $q$ randomly selected requests (unbiased exploration)
    \item \textbf{WorstCost}: Removes requests with highest distance contribution
    \item \textbf{LongestRoute}: Removes from longest routes (targets fairness)
\end{itemize}

\textbf{Repair Operators:}
\begin{itemize}
    \item \textbf{Greedy}: Reinserts using flexible pickup-dropoff construction heuristic
    \item \textbf{RandomGreedy}: Reinserts at random feasible positions
    \item \textbf{ObjectiveAware}: Minimizes full objective including fairness penalty
\end{itemize}

\subsection{Adaptive Weight Mechanism}

Operators are selected via roulette wheel with adaptive weights.
Scores: 10 for new best, 1 for accepted, 0 for rejected.
Weights update every 100 iterations using:
\[
\rho_i \leftarrow \rho_i \cdot (1 - \gamma) + \gamma \cdot \frac{s_i}{a_i}
\]
where $\gamma$ is the reaction factor, $s_i$ total score, and $a_i$ applications in the period.

\subsection{Parameters}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l l}
\toprule
Parameter & Description & Default \\
\midrule
\texttt{max\_iterations} & Maximum iterations & 10000 \\
\texttt{max\_time\_seconds} & Runtime limit & 300s \\
\texttt{weight\_update\_period} & Weight update frequency & 100 \\
\texttt{reaction\_factor} ($\gamma$) & Adaptation speed & 0.1 \\
\texttt{min/max\_removal\_pct} & Removal range & 10\%-40\% \\
\texttt{initial\_temperature} & SA start temperature & 100.0 \\
\texttt{cooling\_rate} & Temperature decay & 0.99 \\
\bottomrule
\end{tabular}
\caption{ALNS default parameters}
\end{table}

% ============================================================
% MAB Operator Selection
% ============================================================
\section{Multi-Armed Bandit Operator Selection}

The adaptive weight mechanism in baseline ALNS has several limitations:
periodic updates introduce delayed feedback, purely proportional selection offers minimal exploration,
and all operators are treated equally regardless of how well-tested they are.
Multi-armed bandit (MAB) algorithms address these issues by providing explicit exploration mechanisms
and immediate learning from each iteration.

\subsection{Method}

Applying MAB to ALNS operator selection requires three key design decisions:
the definition of arms, the reward mechanism, and the learning policy.

\subsubsection{Arms Definition}

Each ALNS iteration requires selecting both a destroy and a repair operator.
Three alternatives were considered for modeling this as a MAB problem:

\begin{enumerate}
    \item \textbf{Destroy operators only}: Model only the 3 destroy operators as arms and use a fixed repair strategy.
    This approach appears in the BALANS paper~\cite{balans}, where repair is done via an exact MIP solver.
    However, we have 3 diverse repair operators that provide complementary search behavior,
    and fixing repair would waste this diversity. 
    Additionally for the sake of clean comparison with the existing adaptive approach, it is beneficial to use the 
    same operators and focus puerly on their selection strategies.  

    \item \textbf{Operator pairs}: Model all $3 \times 3 = 9$ destroy-repair combinations as individual arms.
    This captures potential synergies between specific operators (e.g., WorstCost + Greedy may work differently than Random + Greedy).
    However, 9 arms significantly slows learning compared to 6 arms, and interpreting which combinations work well becomes less clear.

    \item \textbf{Independent selection} (chosen): Maintain two independent MAB instances---one for destroy operators (3 arms)
    and one for repair operators (3 arms). Each iteration selects from both independently.
    This explores the full $3 \times 3$ space with only 6 total arms, accelerating learning while matching baseline ALNS behavior.
\end{enumerate}

The independent selection approach balances exploration efficiency with operator diversity.

\subsubsection{Reward Mechanism}

The reward function defines what constitutes "success" for an operator.
Three options were considered:

\begin{enumerate}
    \item \textbf{Binary (0/1)}: Reward of 1 if the iteration improves the current solution, 0 otherwise.
    This is simple and aligns naturally with Thompson Sampling's Beta-Bernoulli conjugate prior,
    but ignores the magnitude of improvement.

    \item \textbf{Continuous ($\Delta$ objective)}: Reward equals the actual objective improvement $(f_{\text{old}} - f_{\text{new}})$.
    This captures improvement magnitude but requires different priors for Thompson Sampling (e.g., Normal-Gamma)
    and can be sensitive to problem scaling.

    \item \textbf{Tiered (10/1/0)} (chosen): Reward of 10 if the iteration finds a new global best solution,
    1 if it improves the current solution (accepted by simulated annealing), and 0 otherwise.
    This matches the baseline scoring system and emphasizes discovering high-quality solutions,
    not just local improvements.
\end{enumerate}

For Thompson Sampling, which requires binary feedback for Beta distributions,
tiered rewards are converted to binary: any positive reward counts as success.

\subsubsection{Learning Policies}

Two MAB algorithms are compared against the adaptive weight baseline:

\textbf{UCB1 (Upper Confidence Bound)}:
Selects the operator with the highest upper confidence bound:
\[
\text{UCB}_i(t) = \bar{r}_i + c \sqrt{\frac{\ln t}{n_i}}
\]
where $\bar{r}_i$ is the average reward for operator $i$, $t$ is the total number of selections,
$n_i$ is the number of times operator $i$ has been selected, and $c$ is an exploration constant (default 1.0).
The second term provides an explicit exploration bonus: operators that have been selected less frequently
get higher confidence bounds, encouraging balanced exploration.

\textbf{Thompson Sampling}:
Maintains a Beta distribution $\text{Beta}(\alpha_i, \beta_i)$ for each operator $i$,
representing the posterior belief about its success probability.
At each iteration, a sample $\theta_i \sim \text{Beta}(\alpha_i, \beta_i)$ is drawn for each operator,
and the operator with the highest sample is selected.
After observing the outcome, the distribution is updated:
$\alpha_i \leftarrow \alpha_i + 1$ if the reward was positive (success), otherwise $\beta_i \leftarrow \beta_i + 1$.
Thompson Sampling naturally balances exploration and exploitation through probabilistic sampling,
and is known to perform well in non-stationary environments where operator effectiveness may change during the search.

Both algorithms update immediately after each iteration, providing faster adaptation than the baseline's periodic updates.

\subsection{Research Questions}

The experiments aim to answer:
\begin{itemize}
    \item \textbf{RQ1}: Does MAB-based selection outperform adaptive weights in terms of solution quality?
    \item \textbf{RQ2}: Which MAB algorithm (UCB1 or Thompson Sampling) performs best?
    \item \textbf{RQ3}: How does operator selection evolve over time (exploration to exploitation)?
\end{itemize}

\subsection{Results}

% Placeholder for comparison table
\begin{table}[H]
\centering
\caption{Solution quality comparison: Adaptive Weights vs. MAB algorithms}
\label{tab:mab-comparison}
\begin{tabular}{l c c c c}
\toprule
Algorithm & Mean Obj. & Std Dev & Wins & $p$-value \\
\midrule
Adaptive Weights & --- & --- & --- & --- \\
UCB1 & --- & --- & --- & --- \\
Thompson Sampling & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

% Placeholder for convergence plot
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{placeholder_convergence.png}
\caption{Convergence comparison: best objective value over iterations}
\label{fig:convergence}
\end{figure}

% Placeholder for operator evolution plot
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{placeholder_operator_evolution.png}
\caption{Operator selection frequency over time segments (destroy operators)}
\label{fig:operator-evolution}
\end{figure}

% Placeholder for operator performance table
\begin{table}[H]
\centering
\caption{Operator performance: average reward and selection frequency}
\label{tab:operator-performance}
\begin{tabular}{l c c c}
\toprule
Operator & Avg. Reward & Selection \% & Algorithm \\
\midrule
Random & --- & --- & --- \\
WorstCost & --- & --- & --- \\
LongestRoute & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

Statistical significance is assessed using the Wilcoxon signed-rank test (paired, non-parametric)
with significance level $\alpha = 0.05$.

\subsection{Discussion}

% TODO: Add discussion of results after experiments are run

% ============================================================
% RL Operator Selection
% ============================================================
\section{Reinforcement Learning Operator Selection}

% TODO: Describe RL approach for operator selection
% - State representation
% - Action space (destroy operators)
% - Reward function
% - Learning algorithm (Q-learning, DQN, etc.)
% - Results and comparison

\subsection{Method}

\subsection{Results}

% ============================================================
% Comparison
% ============================================================
\section{Comparison and Analysis}

% TODO: Compare all three approaches
% - Adaptive weights (baseline)
% - MAB operator selection
% - RL operator selection
% - Statistical tests (Wilcoxon)
% - Plots showing convergence, operator selection frequencies

% ============================================================
% Conclusion
% ============================================================
\section{Conclusion}

% TODO: Summary of findings
% - Which approach works best and when
% - Trade-offs between methods
% - Lessons learned

% ============================================================
% References
% ============================================================
\begin{thebibliography}{9}

\bibitem{balans}
Cai, J., Kadıoğlu, S., \& Dilkina, B. (2024).
\textit{BALANS: Multi-Armed Bandits-based Adaptive Large Neighborhood Search}.
arXiv preprint arXiv:2410.07393.

\end{thebibliography}

\end{document}