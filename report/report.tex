\documentclass[a4paper]{scrartcl}

\usepackage[a4paper, left=1.8cm, right=1.8cm, top=2.5cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{scrlayer-scrpage}
\usepackage{xcolor}
\usepackage{hyperref}

\renewcommand{\arraystretch}{1.4}
\pagestyle{scrheadings}
\automark{section}
\ohead{\pagemark}
\cfoot{}

% --- Document Info ---
\title{ML-Enhanced Large Neighborhood Search \\ Project Report}
\subtitle{Machine Learning for Optimization, WS 2025}
\author{
    Daniel Levin (12433760)
}
\date{}

\begin{document}
\maketitle

% ============================================================
% Problem Description
% ============================================================
\section{Problem: SCF-PDP}

The \textbf{Selective Capacitated Fair Pickup and Delivery Problem (SCF-PDP)} is a vehicle routing problem
where the goal is to design fair and feasible routes for a subset of customer requests.
Each customer needs transportation of goods from a pickup location to a corresponding drop-off location.

The problem is modeled on a complete directed graph $G = (V, A)$ where:
\begin{itemize}
    \item $V$ contains the vehicle depot plus all pickup/drop-off locations
    \item $A = \{(u,v) : u,v \in V, u \neq v\}$ represents travel routes
    \item Arc distances $a_{u,v} = \lceil\sqrt{(x_u - x_v)^2 + (y_u - y_v)^2}\rceil$ (rounded Euclidean)
\end{itemize}

There are $n$ customer requests $CR = \{1, \ldots, n\}$, each defined by pickup location $v_i^\uparrow$ and
drop-off location $v_i^\downarrow$ with demand $c_i$.
A fleet $K$ of $n_K$ identical vehicles with capacity $C$ serves requests starting from the depot.

A feasible solution assigns routes $R_k$ to each vehicle $k \in K$ such that:
\begin{itemize}
    \item Vehicle capacity never exceeded along any route
    \item Each served request handled entirely by one vehicle
    \item At least $\gamma$ requests served across all vehicles
\end{itemize}

The objective minimizes total distance plus fairness penalty:
\[
\sum_{k \in K} d(R_k) + \rho \cdot (1 - J(R))
\]
where $d(R_k)$ is route $k$'s total distance and $J(R)$ is the Jain fairness index:
\[
J(R) = \frac{\left(\sum_{k \in K} d(R_k)\right)^2}{n_K \cdot \sum_{k \in K} d(R_k)^2}
\]
The fairness index $J(R) \in (0, 1]$ equals 1 when all routes have equal distance.
Parameter $\rho$ controls the distance-fairness trade-off.

% ============================================================
% Baseline ALNS
% ============================================================
\section{Baseline: ALNS}

The baseline implementation uses Adaptive Large Neighborhood Search (ALNS) with simulated annealing acceptance,
developed and tuned in the Heuristic Optimization Techniques course.
This section summarizes the key components.

\subsection{Operators}

The implementation uses 3 destroy and 3 repair operators, yielding 9 combinations.
Each destroy operator removes $q$ requests sampled uniformly from $[10\%, 40\%]$ of served requests.

\textbf{Destroy Operators:}
\begin{itemize}
    \item \textbf{Random}: Removes $q$ randomly selected requests (unbiased exploration)
    \item \textbf{WorstCost}: Removes requests with highest distance contribution
    \item \textbf{LongestRoute}: Removes from longest routes (targets fairness)
\end{itemize}

\textbf{Repair Operators:}
\begin{itemize}
    \item \textbf{Greedy}: Reinserts using flexible pickup-dropoff construction heuristic
    \item \textbf{RandomGreedy}: Reinserts at random feasible positions
    \item \textbf{ObjectiveAware}: Minimizes full objective including fairness penalty
\end{itemize}

\subsection{Adaptive Weight Mechanism}

Operators are selected via roulette wheel with adaptive weights.
Scores: 10 for new best, 1 for accepted, 0 for rejected.
Weights update every 100 iterations using:
\[
\rho_i \leftarrow \rho_i \cdot (1 - \gamma) + \gamma \cdot \frac{s_i}{a_i}
\]
where $\gamma$ is the reaction factor, $s_i$ total score, and $a_i$ applications in the period.

\subsection{Parameters}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l l}
\toprule
Parameter & Description & Default \\
\midrule
\texttt{max\_iterations} & Maximum iterations & 10000 \\
\texttt{max\_time\_seconds} & Runtime limit & 300s \\
\texttt{weight\_update\_period} & Weight update frequency & 100 \\
\texttt{reaction\_factor} ($\gamma$) & Adaptation speed & 0.1 \\
\texttt{min/max\_removal\_pct} & Removal range & 10\%-40\% \\
\texttt{initial\_temperature} & SA start temperature & 100.0 \\
\texttt{cooling\_rate} & Temperature decay & 0.99 \\
\bottomrule
\end{tabular}
\caption{ALNS default parameters}
\end{table}

% ============================================================
% MAB Operator Selection
% ============================================================
\section{Multi-Armed Bandit Operator Selection}

The adaptive weight mechanism in baseline ALNS has several limitations:
periodic updates introduce delayed feedback, purely proportional selection offers minimal exploration,
and all operators are treated equally regardless of how well-tested they are.
Multi-armed bandit (MAB) algorithms address these issues by providing explicit exploration mechanisms
and immediate learning from each iteration~\cite{cai2024balans,phan2024balance}.

\subsection{Method}

Applying MAB to ALNS operator selection requires three key design decisions~\cite{cai2024balans}:
the definition of arms, the reward mechanism, and the learning policy.

\subsubsection{Arms Definition}

Each ALNS iteration requires selecting both a destroy and a repair operator.
Three alternatives were considered for modeling this as a MAB problem:

\begin{enumerate}
    \item \textbf{Destroy operators only}: Model only the 3 destroy operators as arms and use a fixed repair strategy.
    This approach appears in the BALANS paper~\cite{cai2024balans}, where repair is done via an exact MIP solver.
    However, we have 3 diverse repair operators that provide complementary search behavior,
    and fixing repair would waste this diversity. 
    Additionally for the sake of clean comparison with the existing adaptive approach, it is beneficial to use the 
    same operators and focus puerly on their selection strategies.  

    \item \textbf{Operator pairs}: Model all $3 \times 3 = 9$ destroy-repair combinations as individual arms.
    This captures potential synergies between specific operators (e.g., WorstCost + Greedy may work differently than Random + Greedy).
    However, 9 arms significantly slows learning compared to 6 arms, and interpreting which combinations work well becomes less clear.

    \item \textbf{Independent selection} (chosen): Maintain two independent MAB instances---one for destroy operators (3 arms)
    and one for repair operators (3 arms). Each iteration selects from both independently.
    This explores the full $3 \times 3$ space with only 6 total arms, accelerating learning while matching baseline ALNS behavior.
\end{enumerate}

The independent selection approach balances exploration efficiency with operator diversity.

\subsubsection{Reward Mechanism}

The reward function defines what constitutes "success" for an operator.
Three options were considered:

\begin{enumerate}
    \item \textbf{Binary (0/1)}: Reward of 1 if the iteration improves the current solution, 0 otherwise.
    This is simple and aligns naturally with Thompson Sampling's Beta-Bernoulli conjugate prior~\cite{phan2024balance},
    but ignores the magnitude of improvement.

    \item \textbf{Continuous ($\Delta$ objective)}: Reward equals the actual objective improvement $(f_{\text{old}} - f_{\text{new}})$.
    This captures improvement magnitude but requires different priors for Thompson Sampling (e.g., Normal-Gamma)
    and can be sensitive to problem scaling.

    \item \textbf{Tiered (10/1/0)} (chosen): Reward of 10 if the iteration finds a new global best solution,
    1 if it improves the current solution (accepted by simulated annealing), and 0 otherwise.
    This matches the baseline scoring system and emphasizes discovering high-quality solutions,
    not just local improvements.
\end{enumerate}

For Thompson Sampling, which requires binary feedback for Beta distributions,
tiered rewards are converted to binary: any positive reward counts as success.

\subsubsection{Learning Policies}

Two MAB algorithms are compared against the adaptive weight baseline:

\textbf{UCB1 (Upper Confidence Bound)}:
Selects the operator with the highest upper confidence bound:
\[
\text{UCB}_i(t) = \bar{r}_i + c \sqrt{\frac{\ln t}{n_i}}
\]
where $\bar{r}_i$ is the average reward for operator $i$, $t$ is the total number of selections,
$n_i$ is the number of times operator $i$ has been selected, and $c$ is an exploration constant (default 1.0).
The second term provides an explicit exploration bonus: operators that have been selected less frequently
get higher confidence bounds, encouraging balanced exploration.

\textbf{Thompson Sampling}:
Maintains a Beta distribution $\text{Beta}(\alpha_i, \beta_i)$ for each operator $i$,
representing the posterior belief about its success probability.
At each iteration, a sample $\theta_i \sim \text{Beta}(\alpha_i, \beta_i)$ is drawn for each operator,
and the operator with the highest sample is selected.
After observing the outcome, the distribution is updated:
$\alpha_i \leftarrow \alpha_i + 1$ if the reward was positive (success), otherwise $\beta_i \leftarrow \beta_i + 1$.
Thompson Sampling naturally balances exploration and exploitation through probabilistic sampling,
and is known to perform well in non-stationary environments where operator effectiveness may change during the search~\cite{phan2024balance}.

Both algorithms update immediately after each iteration, providing faster adaptation than the baseline's periodic updates.

\subsection{Research Questions}

The experiments aim to answer:
\begin{itemize}
    \item \textbf{RQ1}: Does MAB-based selection outperform adaptive weights in terms of solution quality?
    \item \textbf{RQ2}: Which MAB algorithm (UCB1 or Thompson Sampling) performs best?
    \item \textbf{RQ3}: How does operator selection evolve over time (exploration to exploitation)?
\end{itemize}

\subsection{Results}

Ran 120 instances (30 per size: 50, 100, 200, 500 requests) with 10k iterations each.
Head-to-head comparisons on identical instances.

\textbf{Adaptive vs Thompson} (Table~\ref{tab:adaptive-thompson}, Figure~\ref{fig:adaptive-thompson}):
Thompson wins 67-53 with 0.63\% better total objective.
Mixed results by size: Thompson wins at $n=50$ ($p=0.036$), Adaptive at $n=100$ ($p=0.045$), no significant difference for larger sizes.
Slight edge to Thompson but the baseline stays competitive.

\begin{table}[H]
\centering
\small
\caption{Adaptive Weights vs Thompson Sampling by instance size}
\label{tab:adaptive-thompson}
\begin{tabular}{r r r r r r}
\toprule
Size & Adaptive Obj & Thompson Obj & \% Diff & Wins (A/T) & $p$-value \\
\midrule
50 & $1510 \pm 136$ & $1483 \pm 114$ & $+1.6\%$ & 10/20 & 0.036* \\
100 & $3867 \pm 272$ & $3914 \pm 261$ & $-1.3\%$ & 20/10 & 0.045* \\
200 & $10732 \pm 871$ & $10659 \pm 923$ & $+0.7\%$ & 12/18 & 0.299 \\
500 & $42024 \pm 2572$ & $41712 \pm 2380$ & $+0.7\%$ & 11/19 & 0.164 \\
\midrule
\textbf{Total} & 1,743,960 & 1,733,022 & $+0.63\%$ & 53/67 & --- \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Adaptive_vs_Thompson_50-100-200-500_20260131_201746}
\caption{Adaptive vs Thompson: distribution of \% differences (positive = Thompson better) and win counts}
\label{fig:adaptive-thompson}
\end{figure}

\textbf{Adaptive Weights vs UCB1} (Figure~\ref{fig:adaptive-ucb1}):
Adaptive dominates with 102 wins vs 18, achieving 7.6\% lower total objective.
The gap widens with instance size: from $-0.9\%$ at $n=50$ (not significant) to $-8.6\%$ at $n=500$ ($p < 10^{-9}$).
UCB1's deterministic selection appears to over-exploit early successes, converging prematurely on suboptimal operators.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Adaptive_vs_Ucb1_50-100-200-500_20260131_221456}
\caption{Adaptive vs UCB1: Adaptive significantly outperforms, especially on larger instances}
\label{fig:adaptive-ucb1}
\end{figure}

\textbf{UCB1 vs Thompson Sampling} (Figure~\ref{fig:ucb1-thompson}):
Thompson wins 108 out of 120 instances with 7.5\% better total objective.
The advantage is significant at all sizes ($p < 0.007$) and grows from $+2.7\%$ at $n=50$ to $+8.0\%$ at $n=500$.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Ucb1_vs_Thompson_50-100-200-500_20260201_001945}
\caption{UCB1 vs Thompson: Thompson dominates across all instance sizes}
\label{fig:ucb1-thompson}
\end{figure}

Statistical significance assessed using Wilcoxon signed-rank test ($\alpha = 0.05$).

\subsection{Case Study: Detailed Run Analysis}

Looking at a single instance (\texttt{instance61\_nreq100\_nveh2\_gamma91}, 500 iterations) reveals the behavioral differences.

\textbf{Convergence} (Figure~\ref{fig:convergence}):
All three start at $\sim$4900 but diverge quickly.
Thompson achieves the best result (4184.4, 15.0\% improvement), Adaptive is close behind (4202.5, 14.6\%), but UCB1 plateaus early (4394.0, only 9.8\%).
UCB1 stagnates around iteration 150, while Thompson keeps finding improvements even past iteration 400.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{convergence_instance61_nreq100_nveh2_gamma91}
\caption{Convergence on instance61: Thompson continues improving while UCB1 plateaus early}
\label{fig:convergence}
\end{figure}

\textbf{Operator selection patterns:}

\textbf{Thompson} (Figures~\ref{fig:thompson-destroy},~\ref{fig:thompson-repair}):
Identifies the winning combination early and sticks with it: WorstCost (72\%) + Greedy (87\%).
Frequencies stay stable throughout---no dramatic exploration-to-exploitation shift, just steady probabilistic sampling.

\textbf{UCB1} (Figures~\ref{fig:ucb1-destroy},~\ref{fig:ucb1-repair}):
Locks onto Random destroy (87\%) while barely using WorstCost (6\%)---the wrong choice!
Thompson proves WorstCost is better, but UCB1's deterministic selection locked in after early luck.
Gets repair right (87\% Greedy) but the bad destroy choice kills performance.

\textbf{Adaptive} (Figures~\ref{fig:adaptive-destroy},~\ref{fig:adaptive-repair}):
Maintains balanced diversity: destroy stays at 36\%/33\%/30\%, shifting gradually over time.
Repair adapts more: Greedy grows from 35\% to 45\%.
This diversity explains why Adaptive is competitive---no catastrophic over-commitment like UCB1.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{instance61_nreq100_nveh2_gamma91_Thompson_destroy_evolution}
\caption{Thompson destroy operators}
\label{fig:thompson-destroy}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{instance61_nreq100_nveh2_gamma91_Thompson_repair_evolution}
\caption{Thompson repair operators}
\label{fig:thompson-repair}
\end{minipage}
\end{figure}

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{instance61_nreq100_nveh2_gamma91_UCB1_destroy_evolution}
\caption{UCB1 destroy: locks onto Random}
\label{fig:ucb1-destroy}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{instance61_nreq100_nveh2_gamma91_UCB1_repair_evolution}
\caption{UCB1 repair operators}
\label{fig:ucb1-repair}
\end{minipage}
\end{figure}

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{instance61_nreq100_nveh2_gamma91_Adaptive_destroy_evolution}
\caption{Adaptive destroy: balanced selection}
\label{fig:adaptive-destroy}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{instance61_nreq100_nveh2_gamma91_Adaptive_repair_evolution}
\caption{Adaptive repair: gradual shift}
\label{fig:adaptive-repair}
\end{minipage}
\end{figure}

\subsection{Discussion}

\textbf{Main findings:}
Thompson Sampling wins overall (67-53 vs Adaptive, 108-12 vs UCB1) but the margin over the baseline is small (+0.6\%).
UCB1 performs significantly worse (-7.6\% vs Adaptive) due to premature convergence---it locks onto Random destroy operator which happens to be the wrong choice.
The baseline Adaptive Weights are surprisingly competitive because they maintain operator diversity (30-40\% for each), avoiding UCB1's catastrophic over-exploitation.

\textbf{Why Thompson works:}
Probabilistic sampling naturally balances exploration and exploitation.
Even after identifying WorstCost as the best operator (72\% selection), it keeps sampling alternatives (15\% Random, 12\% LongestRoute).
This prevents UCB1-style lock-in while still exploiting knowledge.

\textbf{Bottom line:}
For this problem, the simple baseline is hard to beat.
Thompson provides marginal gains but adds complexity.
If switching to MAB, only Thompson is viable---UCB1's deterministic selection is too greedy for long ALNS runs.

% ============================================================
% RL Operator Selection
% ============================================================
\section{Reinforcement Learning Operator Selection}

% TODO: Describe RL approach for operator selection
% - State representation
% - Action space (destroy operators)
% - Reward function
% - Learning algorithm (Q-learning, DQN, etc.)
% - Results and comparison

\subsection{Method}

\subsection{Results}

% ============================================================
% Comparison
% ============================================================
\section{Comparison and Analysis}

% TODO: Compare all three approaches
% - Adaptive weights (baseline)
% - MAB operator selection
% - RL operator selection
% - Statistical tests (Wilcoxon)
% - Plots showing convergence, operator selection frequencies

% ============================================================
% Conclusion
% ============================================================
\section{Conclusion}

% TODO: Summary of findings
% - Which approach works best and when
% - Trade-offs between methods
% - Lessons learned

% ============================================================
% References
% ============================================================
\begin{thebibliography}{9}

\bibitem{cai2024balans}
Cai, J., Kadıoğlu, S., \& Dilkina, B. (2024).
\textit{BALANS: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problems}.
arXiv preprint arXiv:2410.07393.

\bibitem{phan2024balance}
Phan, T., Huang, T., Dilkina, B., \& Koenig, S. (2024).
\textit{Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search}.
In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).

\end{thebibliography}

\end{document}