# MAB Experiment Setup - Complete ✅

## Implementation Summary

### Phase 1: Core Infrastructure ✅
- **`src/alns/selectors/base.py`** (56 lines) - OperatorSelector ABC
- **`src/alns/selectors/adaptive_weights.py`** (93 lines) - Baseline selector
- **`src/alns/alns.py`** - Modified to accept optional selectors
  - Generalized `ALNSStatistics` to use selector stats
  - Defaults to `AdaptiveWeightSelector` for backward compatibility

### Phase 2: MAB Implementation ✅
- **`src/alns/selectors/mab.py`** (156 lines total)
  - **UCB1Selector** - Formula: `UCB(i) = μᵢ + c·√(2·ln(t)/nᵢ)`, c=1.0
  - **ThompsonSamplingSelector** - Beta-Bernoulli with binary reward conversion
- Both follow paper formulations exactly
- YAGNI applied: removed unused `reset()` methods

### Phase 3: Experiment Framework ✅

#### Statistical Comparison (RQ1, RQ2)
**`src/experiments/mab_comparison.py`** (150 lines)
- Reuses `src/algorithm_comparison.py` framework
- Compares: Adaptive vs UCB1, Adaptive vs Thompson, UCB1 vs Thompson
- Generates:
  - Wilcoxon signed-rank tests
  - Boxplots of objective distributions
  - Win/tie/loss counts
  - Auto-saved to `plots/`

#### Detailed Analysis (RQ4)
**`src/experiments/run_detailed_experiment.py`** (160 lines)
- Runs experiments and saves full `ALNSStatistics`
- Pickles statistics for offline analysis
- Outputs to `results/mab_detailed/`

#### Convergence Analysis (RQ1, RQ2)
**`src/experiments/plot_convergence.py`** (100 lines)
- Plots iteration vs best objective
- Compares convergence speed across algorithms
- Multiple instances support

#### Operator Evolution (RQ4)
**`src/experiments/plot_operator_evolution.py`** (220 lines)
- Stacked bar charts of operator selection over time
- Shows exploration → exploitation transition
- Computes average reward per operator
- Separate plots for destroy and repair operators

## Research Questions Mapping

| RQ | Metric | Tool |
|----|--------|------|
| **RQ1**: MAB vs Baseline | Final objective, p-values | `mab_comparison.py` |
| **RQ2**: Which MAB best? | Statistical comparison | `mab_comparison.py` |
| **RQ4**: Selection evolution | Frequency over time, avg reward | `plot_operator_evolution.py` |

## How to Run Experiments

### Option A: Quick Statistical Comparison
```bash
python -m src.experiments.mab_comparison
```
- Runs on configured instances
- Outputs plots to `plots/`
- Uses existing `algorithm_comparison.py` framework

### Option B: Detailed Analysis Pipeline
```bash
# Step 1: Run experiments and save statistics
python -m src.experiments.run_detailed_experiment

# Step 2: Generate convergence plots
python -m src.experiments.plot_convergence

# Step 3: Generate operator evolution plots
python -m src.experiments.plot_operator_evolution
```

## Configuration Points

All scripts have clear configuration sections at `if __name__ == "__main__"`:

```python
# In mab_comparison.py
instance_sizes = ["50", "100", "200"]  # Configure here
max_iterations = 1000

# In run_detailed_experiment.py
instance_paths = [
    Path("scfpdp_instances/50/test_instance_n50_01.txt"),
    # Add more...
]
config = ALNSConfig(max_iterations=1000, max_time_seconds=300.0)
```

## Output Structure

```
plots/
├── Adaptive_vs_Thompson_50-100-200_<timestamp>.png  # Comparison
├── Adaptive_vs_Ucb1_50-100-200_<timestamp>.png
├── convergence/
│   └── convergence_<instance>_<algorithm>.png
└── operator_evolution/
    ├── <instance>_<algorithm>_destroy_evolution.png
    └── <instance>_<algorithm>_repair_evolution.png

results/
└── mab_detailed/
    ├── summary.pkl
    ├── <instance>_Adaptive.pkl
    ├── <instance>_Ucb1.pkl
    └── <instance>_Thompson.pkl
```

## For Report: Expected Deliverables

### Tables (From mab_comparison.py output)
1. **Algorithm Comparison** - Median/Mean/Std/p-value per instance size
2. **Operator Performance** - Can extract from saved statistics

### Plots
1. **Convergence Comparison** - `plot_convergence.py`
2. **Solution Quality Boxplot** - Auto-generated by `mab_comparison.py`
3. **Operator Selection Evolution** - `plot_operator_evolution.py` (destroy + repair)

## Key Design Decisions Implemented

| Decision | Implementation |
|----------|----------------|
| Reward function | Tiered (10/1/0) in ALNS, binary conversion for Thompson |
| Exploration parameter (UCB1) | c = 1.0 (paper default) |
| Thompson Sampling | Beta-Bernoulli with binary reward |
| Selector scope | Both destroy and repair independently |
| Statistics tracking | Delegated to selectors via `get_statistics()` |

## Code Statistics

**Total new code**: ~850 lines
**Modified code**: ~40 lines in `alns.py`

**Breakdown**:
- Selectors: 316 lines (base + adaptive + MAB)
- Experiments: 630 lines (comparison + runner + plotting)
- Documentation: ~100 lines (READMEs)

## Next Steps for User

1. **Configure instance paths** in experiment scripts
2. **Run mab_comparison.py** for statistical comparison (RQ1, RQ2)
3. **Run detailed experiment pipeline** for RQ4 analysis
4. **Review plots** and extract data for report tables
5. **Analyze results** and write report sections

All infrastructure is ready. Just need to point it at the right instances and let it run!